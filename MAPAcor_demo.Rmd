---
title: "MAPA Functions"
author: "Ishaan Gadiyar"
date: "`r Sys.Date()`"
output:
  html_document: 
    code_folding: hide
    toc: true
    toc_float: true
    number_sections: true
    theme: spacelab
    highlight: haddock
---

# Introduction
The aim of this tutorial is to show users how to download the *MAPA* package and how to appropriately use the functions available in the package. While this specific tutorial uses the *Reproducible Brain Charts (RBC)* dataset (https://reprobrainchart.github.io/) to train and evaluate random forest and ridge regression models, *MAPA* can be used across a variety of black-box models that predict continuous outcomes. 

# Loading Packages/Libraries
```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(knitr) 
library(glmnet)
library(locfit)
library(ggplot2)
library(ranger)
library(tuneRanger)
library(mlr)    

# Installing and loading MAPA package
if (!"semicor" %in% rownames(installed.packages())) {
  devtools::install_github("statimagcoll/semicor", upgrade = "never")
}
library(semicor)

# Setting the seed
set.seed(123)

# Remove this later
source("/media/disk2/multivariateBWAS/code_for_review/build_funcs.R")
# Sourcing influence and correlation functions
source("/media/disk2/multivariateBWAS/code_for_review/influence_funcs.R")
source("/media/disk2/multivariateBWAS/code_for_review/corr_funcs.R")
```

# Methods
## mapaFolds: A Cross-Fitting Function
Here, *mapaFolds* is a function that handles cross-fitting for a variety of models. It can be called directly from the *MAPA* package, and the code is provided below. 
```{r}
mapaFolds <- function(y, y_hat, folds, y_hat2 = NULL, level = 0.95){
  # Checking that variable lengths match
  if(length(y_hat) != length(y) | length(y_hat) != length(folds) | length(y) != length(folds)){
    stop("y, y_hat, and folds must be of the same length.")
  }
  
  # Creating storage vector for results and variables
  os_folds = c()
  inf_folds = c()
  n = length(y)
  
  # Looping through folds
  for(fold in sort(unique(folds))){
    # Calculating and storing OS estimator
    os_res = cor_os(y_hat[which(folds == fold)], y[which(folds == fold)])
    os_folds = c(os_folds, os_res)
    
    # Calculating and storing the influence vector
    inf_res = if_logit_sq(y_hat[which(folds == fold)], y[which(folds == fold)])
    inf_folds = c(inf_folds, inf_res)
  }
  
  # Calculating the mean OS estimate across folds
  os_estimate = mean(os_folds, na.rm = T)
  
  # Calculating the logit-scale estimate across folds
  os_estimate_logit = logit(os_estimate^2)
  
  # Calculating the standard error of the OS estimate
  stand_error = sqrt(var(inf_folds)/n)
  
  # Calculating the bounds of the OS estimate
  LB_est = sqrt(expit(os_estimate_logit - qnorm((1 + level) / 2)*stand_error))
  UB_est = sqrt(expit(os_estimate_logit - qnorm((1 - level) / 2)*stand_error))
  
  # Denoting the fold number that each OS fold estimate belongs to
  names(os_folds) <- sort(unique(folds))
  
  # Creating a results data frame for output 
  estimate = data.frame(est = os_estimate, est_logit = os_estimate_logit, LB = LB_est, UB = UB_est, se = stand_error)
    
    
  if(!is.null(y_hat2)){
    # Checking that variable lengths match
    if(length(y_hat2) != length(y) | length(y_hat2) != length(folds) | length(y) != length(folds)){
      stop("y, y_hat2, and folds must be of the same length.")
    }
    
    # Creating storage vector for results and variables
    os_folds2 = c()
    inf_folds2 = c()
    n = length(y)
    
    # Looping through folds
    for(fold in sort(unique(folds))){
      # Calculating and storing OS estimator
      os_res = cor_os(y_hat2[which(folds == fold)], y[which(folds == fold)])
      os_folds2 = c(os_folds2, os_res)
      
      # Calculating the influence vector and storing
      inf_res = if_logit_sq(y_hat2[which(folds == fold)], y[which(folds == fold)])
      inf_folds2 = c(inf_folds2, inf_res)
    }
    
    # Calculating the mean OS estimate across folds
    os_estimate2 = mean(os_folds2, na.rm = T)
    
    # Calculating the logit-scale estimate across folds
    os_estimate2_logit = logit(os_estimate2^2)
    
    # Calculating standard error
    stand_error2 = sqrt(var(inf_folds2)/n)
    
    # Calculating the OS estimate bounds across folds 
    LB_est2 = sqrt(expit(os_estimate2_logit - qnorm((1 + level) / 2)* stand_error2))
    UB_est2 = sqrt(expit(os_estimate2_logit - qnorm((1 - level) / 2)*stand_error2))
    
    # Denoting the fold number that each OS fold estimate belongs to
    names(os_folds2) <- sort(unique(folds))
    
    # Creating data frame output 
    estimate2 = data.frame(est = os_estimate2, est_logit = os_estimate2_logit, LB = LB_est2, UB = UB_est2, se = stand_error2)
    
    # Calculating OS difference - first yhat vs. second yhat
    diff = cor_os_diff(os_estimate, os_estimate2, inf_folds, inf_folds2)
    os_diff = data.frame(est = diff[1], est_logit = NA, LB = diff[2], UB = diff[3], se = diff[4])
    
    # Calculating the OS ratio - first yhat vs. second yhat
    ratio = cor_os_ratio(os_estimate, os_estimate2, inf_folds, inf_folds2)
    os_ratio = data.frame(est = ratio[1], est_logit = NA, LB = ratio[2], UB = ratio[3], se = ratio[4])
    
    # Combining all data into one
    est_total = rbind(estimate, estimate2, os_diff, os_ratio)
    
    # Assigning row names
    est_total$metric <- c("yhat1", "yhat2", "Diff", "Ratio")
    rownames(est_total) <- NULL 
    
    
    # Combining all outputs into one list
    out = list(est = est_total, fold.est = os_folds, fold.est2 = os_folds2, n = n, conf.level = level) # data frame version
  }
  else{
    # Combining all outputs into one list when there is no yhat2
    out = list(est = estimate, fold.est = os_folds, n = n, conf.level = level) 
    
    # Naming the metric being measured in estimate
    out$est$metric = "yhat1"
  }
  
  # Returning the data frame
  return(out)
}
```

## mapa: A Sample-Split Function
Here, *mapa* allows calls on the *mapaFolds* function and allows users to perform sample splits on top of cross-fitting. It can be called directly from the *MAPA* package, and the code is provided below. 
```{r}
mapa <- function(y, y_hat, folds, y_hat2 = NULL, level = 0.95){
  # Turning y into a matrix
  y = as.matrix(y)
  
  # Turning data frames into matrices
  if(is.data.frame(y_hat)){
    y_hat = as.matrix(y_hat)
  }
  
  if(is.data.frame(folds)){
    folds = as.matrix(folds)
  }
  
  if(is.data.frame(y_hat2)){
    y_hat2 = as.matrix(y_hat2)
  }
  
  # Sanity checks
  if(nrow(y) != nrow(y_hat) | nrow(y) != nrow(folds) | nrow(folds) != nrow(y_hat)){
    stop("y and y_hat must have the same amount of rows")
  }
  
  if(ncol(folds) != ncol(y_hat)){
    stop("folds and y_hat must have the same column number")
  }
  
  # Sanity checks for second mu
  if(!is.null(y_hat2)){
      if(ncol(folds) != ncol(y_hat2)){
      stop("folds and y_hat2 must have the same column number")
    }
    
    if(nrow(y) != nrow(y_hat2) | nrow(folds) != nrow(y_hat2)){
      stop("y and y_hat2 must have the same amount of rows")
    }
  }
  
  
  # Assigning the number of sample splits
  n_splits <- ncol(y_hat)
  

  # Gathering results 
  res_list <- lapply(seq_len(n_splits), function(i) {
    tmp <- mapaFolds(y, y_hat[, i], folds[, i], y_hat2[,i], level)
    tmp$est$split <- i
    
    list(
      est   = tmp$est,
      fold1 = tmp$fold.est,
      fold2 = if (is.null(y_hat2)) NULL else tmp$fold.est2
    )
  })
  
  # Combining all results 
  out <- list(
    est        = do.call(rbind, lapply(res_list, `[[`, "est")),
    est1.folds = do.call(rbind, lapply(res_list, `[[`, "fold1")),
    n          = length(y),
    conf.level = level
  )
  
  # Naming rows in est1.folds output to signal sample split number
  rownames(out$est1.folds) <- paste0("ss", seq_len(nrow(out$est1.folds)))
  
  if (!is.null(y_hat2)) {
    # Adding est2.folds when yhat2 is present 
    out$est2.folds <- do.call(rbind, lapply(res_list, `[[`, "fold2"))
    
    # Naming rows in est2.folds output to signal sample split number
    rownames(out$est2.folds) <- paste0("ss", seq_len(nrow(out$est2.folds)))
  }
  
  # Adding a summary data frame element in case there is more than one sample split
  out$est_summary = out$est %>%
    group_by(metric) %>%
    summarise(
      est = median(est, na.rm = TRUE),
      est_logit = median(est_logit, na.rm = TRUE),
      LB = median(LB, na.rm = TRUE),
      UB = median(UB, na.rm = TRUE),
      se = median(se, na.rm = TRUE)
    )
  
  # Storing est_summary as a data frame instead of a tibble
  out$est_summary = data.frame(out$est_summary)
  
  # Returning output
  return(out)
}
```


# Installing Data 
Here, we utilize data from the *Reproducible Brain Charts (RBC)* data set, accessible via the *datalad* package in R (https://reprobrainchart.github.io/docs/get_data). More specifically, this tutorial will use data from the *PNC1* study site within the RBC data set, a subset of data taken from the *RBC* data set. 

In *pnc_data*, the pertinent data that we use are: 

- Age: a continuous measure in years that represents how old a subjectâ€™s brain is. We predict this measure in both model examples. 
- X17: columns that contain 'X17' represent structural brain measures, or variables that give a measure of the volume of specific regions within the brain. 
- _to_: columns that contain '_to_' represent functional brain measures, or variables that give a measure of brain activity within specicif regions of the brain.  
- sex_bin: represents the sex of the subject, where 1 represents a male and 0 represents a female. 
- euler: represents the Euler number, which is used to assess the quality of structural MRI scans. 
- meanFD: represents the mean frame displacment rate, which is used to assess the quality of function MRI scans. 
```{r}
# Loading in the structural data - denoted by column names with 'X17'
rbc <- read_csv("/media/disk2/RBC_version0.1/regional_GMV_all_sites_harmonized_covariates.csv")

# Loading in the functional data - denoted by column names with '_to_'
func <- read_csv("/media/disk2/RBC_version0.1/FC_network17_harmonized_covariates.csv")

# Setting each dataset to only include patients that overlap 
rbc = rbc[rbc$participant_id %in% func$participant_id,]
func = func[func$participant_id %in% rbc$participant_id,]

# Checking that only overlapping participants exist 
sum(rbc$participant_id == func$participant_id) == nrow(func)

# Merging datasets
cols_to_add <- setdiff(names(func), names(rbc))
total_data <- cbind(rbc, func[cols_to_add])

# Subsetting study site to HBNsiteSI
pnc_data = total_data %>% filter(study_site == "PNC1")

# Turning the sex column into a binary stored in 'sex_bin'
pnc_data <- pnc_data %>%
  mutate(sex_bin = ifelse(pnc_data$sex == "Male", 1, 0))
```

# Ridge Regression Model Examples
This section seeks to highlight how the *MAPA* package can be used to evaluate ridge regression models in two instances where 1) a model is only trained on cross-fold validation or 2) a model is trained with cross-fold validation and sample splits. 

## Ridge Regression Example with only Cross-Fold Validation
### Training the Model
Here, we test how the inclusion of either structural or functional MRI data in a ridge regression model impacts model accuracy when predicting age. 
```{r}
# Cleaning data - drop NA before cross-fold validation
data_clean = pnc_data %>% drop_na(age) 

# Instantiating number of folds and assigning fold number randomly 
folds = 5
data_clean$fold <- sample(rep(1:folds, length.out = nrow(data_clean)))

# Sanity check for fold distribution 
table(data_clean$fold)

# Creating storage variables for lambda (a finetuning variable)
lambda_struct = c()
lambda_func = c()

# Creating storage columns for model predictions 
data_clean$preds_struct = NA
data_clean$preds_func = NA

# Creating a for loop for 5-fold cross validation
for(i in 1:folds){
  # Assigning data
  test = data_clean %>% filter(fold == i)
  train = data_clean %>% filter(fold != i)
  
  # Creating model data variables for training  
  x_var_struct = as.matrix(train[, grepl('X17', colnames(train)) | colnames(train) %in% c("sex_bin", "euler", "meanFD")])
  y_var = as.matrix(train[, colnames(train) %in% c("age")])
  x_var_func = as.matrix(train[, grepl('_to_', colnames(train)) | colnames(train) %in% c("sex_bin", "euler", "meanFD")])

  # Creating model variables for prediction
  newx_struct = as.matrix(test[, grepl('X17', colnames(test)) | colnames(test) %in% c("sex_bin", "euler", "meanFD")])
  newx_func = as.matrix(test[, grepl('_to_', colnames(test)) | colnames(test) %in% c("sex_bin", "euler", "meanFD")])
  newy = as.matrix(test[, colnames(test) %in% c("age")])
  
  # Fitting a ridge regression model
  lambda_seq <- c(
    seq(10, 1, -1),        # From 10 to 1 (step of 1)
    seq(0.9, 0.1, -0.1),   # From 0.9 to 0.1 (step of 0.1)
    seq(0.09, 0.01, -0.01),# From 0.09 to 0.01 (step of 0.01)
    seq(0.009, 0.001, -0.001), # From 0.009 to 0.001 (step of 0.001)
    seq(0.0009, 0.0001, -0.0001), # From 0.0009 to 0.0001 (step of 0.0001)
    seq(0.00009, 0.00001, -0.00001), # From 0.00009 to 0.00001
    seq(0.000009, 0.000001, -0.000001), # From 0.000009 to 0.000001
    seq(0.0000009, 0.0000001, -0.0000001), # From 0.0000009 to 0.0000001
    0
  )
  
  # Training respective models
  ridgeModel_struct <- cv.glmnet(x_var_struct, y_var, alpha = 0, lambda = lambda_seq)
  ridgeModel_func <- cv.glmnet(x_var_func, y_var, alpha = 0, lambda = lambda_seq)
  
  # Getting predictions
  test$preds_struct = predict(ridgeModel_struct, newx = newx_struct, s = "lambda.min")[,]
  test$preds_func = predict(ridgeModel_func, newx = newx_func, s = "lambda.min")[,]
  
  # Storing prediction values
  ## Isolating to a singular data frame with only the prediction values and IDs
  test_collapsed <- test %>%
  group_by(participant_id) %>%
  summarise(
    preds_struct = first(na.omit(preds_struct)),
    preds_func = first(na.omit(preds_func)),
    .groups = "drop"
  )
  
  ## Matching the predictions by ID to the original data frame
  data_clean <- data_clean %>%
  left_join(test_collapsed, by = "participant_id", suffix = c("", "_new"))%>%
  mutate(
    preds_struct = ifelse(is.na(preds_struct_new), preds_struct, preds_struct_new),
    preds_func = ifelse(is.na(preds_func_new), preds_func, preds_func_new)
  ) %>%
  select(-ends_with("_new"))
  
  
  # Storing minimum lambda used for each ridge regression model
  lambda_struct = c(lambda_struct, ridgeModel_struct$lambda.min)
  lambda_func = c(lambda_func, ridgeModel_func$lambda.min)
}
```

### Applying mapa to Data Predictions
```{r}
# Gathering the estimates with one sample split
results = mapa(as.matrix(data_clean$age), as.matrix(data_clean$preds_struct), as.matrix(data_clean$fold), as.matrix(data_clean$preds_func))

# Here, yhat1 represents the prediction accuracy of the structural model and 
# yhat2 represents the prediction accuracy of the functional model 
results$est <- results$est %>% mutate(metric = recode(metric, yhat1 = "Structural Data", yhat2 = "Functional Data"))
results$est_summary <- results$est_summary %>% mutate(metric = recode(metric, yhat1 = "Structural Data", yhat2 = "Functional Data"))

# Previewing summarized results
results$est_summary
```

### Graphing Outputs
```{r}
# Subsetting results to only estimates
toPlot = results$est_summary %>% filter(metric == "Structural Data" | metric == "Functional Data")
toPlot_Diff = results$est_summary %>% filter(metric == "Diff")

# Plotting one-step results 
ggplot(toPlot, aes(x = metric, y = est)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = LB, ymax = UB),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1)) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")  + 
  labs(x = "Ridge Regression Model Data", y = "One-Step Estimator Value") 

# Plotting one-step difference results 
ggplot(toPlot_Diff, aes(x = metric, y = est)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = LB, ymax = UB),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  theme_bw() + 
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + 
  coord_flip() + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")  + 
  ylim(-1,1) + 
  labs(x = "One-Step Difference", y = "Difference Magnitude")
```


### Analysis 
Here, we see that the ridge regression model trained with structural MRI data performs significantly better than the model trained with functional MRI data when predicting the continuous variable age. 

## Ridge Regression Example with Sample Splits
### Training the Model
Here, like section 4.1, we test how the inclusion of either structural or functional MRI data in a ridge regression model impacts model accuracy when predicting age. However, this time we incorporate sample splits.
```{r}
# Cleaning data
data_clean = pnc_data %>% drop_na(age)

# Assigning sample split number and fold number 
ss_num = 20
folds = 5

# Creating storage variables for hypertuning
lambda_struct = c()
lambda_func = c()

# Assigning storage matrices within data frame columns 
data_clean$struct_ss  <- matrix(nrow = nrow(data_clean), ncol = 0)
data_clean$func_ss  <- matrix(nrow = nrow(data_clean), ncol = 0)
data_clean$fold_ss  <- matrix(nrow = nrow(data_clean), ncol = 0)


for(j in 1:ss_num){
  # Creating a temporary data frame
  data_temp <- data_clean
  
  # Creating storage columns
  data_temp$preds_struct = NA
  data_temp$preds_func = NA
  
  # Randomly assigning fold numbers within the temporary data frame 
  data_temp$fold <- sample(rep(1:folds, length.out = nrow(data_temp)))
  
  # Creating a for loop for 5-fold cross validation
  for(i in 1:folds){
    # Assigning data
    test = data_temp %>% filter(fold == i)
    train = data_temp %>% filter(fold != i)
    
    # Creating model data variables for training  
    x_var_struct = as.matrix(train[, grepl('X17', colnames(train)) | colnames(train) %in% c("sex_bin", "euler", "meanFD")])
    y_var = as.matrix(train[, colnames(train) %in% c("age")])
    x_var_func = as.matrix(train[, grepl('_to_', colnames(train)) | colnames(train) %in% c("sex_bin", "euler", "meanFD")])
  
    # Creating model variables for prediction
    newx_struct = as.matrix(test[, grepl('X17', colnames(test)) | colnames(test) %in% c("sex_bin", "euler", "meanFD")])
    newx_func = as.matrix(test[, grepl('_to_', colnames(test)) | colnames(test) %in% c("sex_bin", "euler", "meanFD")])
    newy = as.matrix(test[, colnames(test) %in% c("age")])
    
    # Fitting a ridge regression model
    lambda_seq <- c(
      seq(10, 1, -1),        # From 10 to 1 (step of 1)
      seq(0.9, 0.1, -0.1),   # From 0.9 to 0.1 (step of 0.1)
      seq(0.09, 0.01, -0.01),# From 0.09 to 0.01 (step of 0.01)
      seq(0.009, 0.001, -0.001), # From 0.009 to 0.001 (step of 0.001)
      seq(0.0009, 0.0001, -0.0001), # From 0.0009 to 0.0001 (step of 0.0001)
      seq(0.00009, 0.00001, -0.00001), # From 0.00009 to 0.00001
      seq(0.000009, 0.000001, -0.000001), # From 0.000009 to 0.000001
      seq(0.0000009, 0.0000001, -0.0000001), # From 0.0000009 to 0.0000001
      0
    )
    
    # Training respective models
    ridgeModel_struct <- cv.glmnet(x_var_struct, y_var, alpha = 0, lambda = lambda_seq)
    ridgeModel_func <- cv.glmnet(x_var_func, y_var, alpha = 0, lambda = lambda_seq)
    
    # Getting predictions
    test$preds_struct = predict(ridgeModel_struct, newx = newx_struct, s = "lambda.min")[,]
    test$preds_func = predict(ridgeModel_func, newx = newx_func, s = "lambda.min")[,]
    
    # Storing prediction values
    ## Isolating to a singular data frame with only the prediction values and IDs
    test_collapsed <- test %>%
      group_by(participant_id) %>%
      summarise(
        preds_struct = first(na.omit(preds_struct)),
        preds_func = first(na.omit(preds_func)),
        .groups = "drop"
      )
    
    ## Matching the predictions by ID to the original data frame
    data_temp <- data_temp %>%
      left_join(test_collapsed, by = "participant_id", suffix = c("", "_new"))%>%
      mutate(
        preds_struct = ifelse(is.na(preds_struct_new), preds_struct, preds_struct_new),
        preds_func = ifelse(is.na(preds_func_new), preds_func, preds_func_new)
      ) %>%
      select(-ends_with("_new"))
    
    
    # Storing minimum lambda used for each ridge regression model
    lambda_struct = c(lambda_struct, ridgeModel_struct$lambda.min)
    lambda_func = c(lambda_func, ridgeModel_func$lambda.min)
  }
  
  # Storing results from each model 
  data_clean$struct_ss <- cbind(data_clean$struct_ss, as.matrix(data_temp$preds_struct))
  data_clean$func_ss <- cbind(data_clean$func_ss, as.matrix(data_temp$preds_func))
  data_clean$fold_ss <- cbind(data_clean$fold_ss, as.matrix(data_temp$fold))
}
```

### Applying mapa to Data Predictions
```{r}
# Gathering the estimates with four sample splits
results = mapa(data_clean$age, data_clean$struct_ss, data_clean$fold_ss, data_clean$func_ss)

# Here, yhat1 represents the prediction accuracy of the structural model and 
# yhat2 represents the prediction accuracy of the functional model 
results$est <- results$est %>% mutate(metric = recode(metric, yhat1 = "Structural Data", yhat2 = "Functional Data"))
results$est_summary <- results$est_summary %>% mutate(metric = recode(metric, yhat1 = "Structural Data", yhat2 = "Functional Data"))

# Previewing summarized results
results$est_summary
```

### Graphing Outputs
Here, we take the median of the estimate and bound values from each sample split to determine what one-step estimator, lower-bound, and upper-bound value to plot.
```{r}
# Filtering to only the estimates for the structural and functional estimates
toPlot <- results$est_summary %>% filter(metric == "Structural Data" | metric == "Functional Data")
toPlot_Diff <- results$est_summary %>% filter(metric == "Diff")

# Plotting results
ggplot(toPlot, aes(x = metric, y = est)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = LB, ymax = UB),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1)) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")  + 
  labs(x = "Ridge Regression Model Data", y = "One-Step Estimator Value")


# Plotting one-step difference results 
ggplot(toPlot_Diff, aes(x = metric, y = est)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = LB, ymax = UB),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  theme_bw() + 
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + 
  coord_flip() + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")  + 
  ylim(-1,1) + 
  labs(x = "One-Step Difference", y = "Difference Magnitude")
```

### Analysis 
Here, even with the inclusion of sample splits to test other scenarios where fold assignment randomly differs, we still see that the model trained on structural data performs significantly better than the model trained on functional data when predicting subject age. 

# Random Forest Model Examples
This section seeks to highlight how the *MAPA* package can be used to evaluate random forest models in two instances where 1) a model is only trained on cross-fold validation or 2) a model is trained with cross-fold validation and sample splits. 

## Methods
Here, outside of the random forest model package itself, we create the *helper_ranger* function to help tune and check the validity of our model tuning. 
```{r}
# tuning function
helper_ranger = function(trainDat, outcome, testDat, task){
  # If the tuned model already gives non-constant preds, keep it.
  if (sd(predict(task$model, newdata = data.frame(testDat))$data$response) > 1e-5) {
    return(task$model)
  }

  # Otherwise walk the tried hyperparams (best MSE first) and re-train via mlr
  tune_table <- task$results[order(task$results$mse), ]

  for (i in seq_len(nrow(tune_table))) {
    params <- tune_table[i, ]

    # build an mlr learner with these params
    lrn <- mlr::makeLearner(
      "regr.ranger",
      num.trees       = 250,
      mtry            = params$mtry,
      min.node.size   = params$min.node.size,
      sample.fraction = params$sample.fraction,
      num.threads     = 1,
      respect.unordered.factors = "order"  # or as needed
    )

    # train an mlr model so predict() keeps the same API
    tr_task <- mlr::makeRegrTask(id = "rf_task_retry", data = trainDat, target = outcome)
    wm <- mlr::train(lrn, tr_task)

    # check for non-constant predictions
    if (sd(predict(wm, newdata = data.frame(testDat))$data$response) > 1e-5) {
      return(wm)
    }
  }

  # if everything fails, just return the original
  return(task$model)
}
```

## Random Forest Example with only Cross-Fold Validation
### Training the Model
Here, we test how the inclusion of either structural or functional data in a random forest model impacts model accuracy when predicting age. 
```{r}
# Cleaning data - drop NA before CV
data_clean = pnc_data %>% drop_na(age) 

# Assigning folds randomly
folds = 5
data_clean$fold <- sample(rep(1:folds, length.out = nrow(data_clean)))

# Selecting an outcome variable 
outcome = "age"

# Sanity check for fold distribution 
table(data_clean$fold)

# Creating storage columns
data_clean$preds_struct = NA
data_clean$preds_func = NA

# Creating a for loop for 5-fold cross validation
for(i in 1:folds){
  # Assigning data
  test = data_clean %>% filter(fold == i)
  train = data_clean %>% filter(fold != i)
  
  # Creating model data variables for training  
  x_var_struct = data.frame(train[, grepl('X17', colnames(train)) | colnames(train) %in% c(outcome, "sex_bin", "euler", "meanFD")])
  y_var = data.frame(train[, colnames(train) %in% c("age")])
  x_var_func = data.frame(train[, grepl('_to_', colnames(train)) | colnames(train) %in% c(outcome, "sex_bin", "euler", "meanFD")])

  # Creating model variables for prediction
  newx_struct = data.frame(test[, grepl('X17', colnames(test)) | colnames(test) %in% c("sex_bin", "euler", "meanFD")])
  newx_func = data.frame(test[, grepl('_to_', colnames(test)) | colnames(test) %in% c("sex_bin", "euler", "meanFD")])
  newy = data.frame(test[, colnames(test) %in% c("age")])
  
  # Creating random forest tasks
  task_struct = makeRegrTask(id = "rf_task", data = x_var_struct, target = outcome)
  task_func = makeRegrTask(id = "rf_task", data = x_var_func, target = outcome)
  
  # Tuning each model
  rfTune_struct = tuneRanger(task_struct, num.trees = 250, iters = 25, iters.warmup = 10, show.info = FALSE)
  rfTune_func = tuneRanger(task_func, num.trees = 250, iters = 25, iters.warmup = 10, show.info = FALSE)
  
  # Checking the tuning of each model
  rfTune_struct$model = helper_ranger(x_var_struct, outcome, newx_struct, rfTune_struct) # Calling for checking of tuning 
  rfTune_func$model = helper_ranger(x_var_func, outcome, newx_func, rfTune_func) # Calling for checking of tuning 
  
  # Getting predictions
  test$preds_struct = predict(rfTune_struct$model, newdata = newx_struct)$data$response
  test$preds_func = predict(rfTune_func$model, newdata = newx_func)$data$response
  
  # Storing prediction values
  ## Isolating to a singular data frame with only the prediction values and IDs
  test_collapsed <- test %>%
  group_by(participant_id) %>%
  summarise(
    preds_struct = first(na.omit(preds_struct)),
    preds_func= first(na.omit(preds_func)),
    .groups = "drop"
  )
  
  ## Matching the predictions by ID to the original data frame
  data_clean <- data_clean %>%
  left_join(test_collapsed, by = "participant_id", suffix = c("", "_new"))%>%
  mutate(
    preds_struct = ifelse(is.na(preds_struct_new), preds_struct, preds_struct_new),
    preds_func = ifelse(is.na(preds_func_new), preds_func, preds_func_new)
  ) %>%
  select(-ends_with("_new"))
}
```

### Applying mapa to Data Predictions
```{r}
# Gathering the estimates with one sample split
results = mapa(as.matrix(data_clean$age), as.matrix(data_clean$preds_struct), as.matrix(data_clean$fold), as.matrix(data_clean$preds_func))

# Here, yhat1 represents the prediction accuracy of the structural model and 
# yhat2 represents the prediction accuracy of the functional model 
results$est <- results$est %>% mutate(metric = recode(metric, yhat1 = "Structural Data", yhat2 = "Functional Data"))
results$est_summary <- results$est_summary %>% mutate(metric = recode(metric, yhat1 = "Structural Data", yhat2 = "Functional Data"))

# Previewing summarized results
results$est_summary
```

### Graphing Outputs
```{r}
# Subsetting results to only estimates
toPlot = results$est_summary %>% filter(metric == "Structural Data" | metric == "Functional Data")
toPlot_Diff =  results$est %>% filter(metric == "Diff")

# Plotting one-step results 
ggplot(toPlot, aes(x = metric, y = est)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = LB, ymax = UB),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1)) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")  + 
  labs(x = "Random Forest Model Data", y = "One-Step Estimator Value")


# Plotting one-step difference results 
ggplot(toPlot_Diff, aes(x = metric, y = est)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = LB, ymax = UB),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  theme_bw() + 
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + 
  coord_flip() + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")  + 
  ylim(-1,1) + 
  labs(x = "One-Step Difference", y = "Difference Magnitude")
```

### Analysis 
Here, we see that the model trained on structural data performs significantly better than the model trained on functional data when predicting subject age. 

## Random Forest Example with Sample Splits
### Training the Model
```{r}
# Cleaning data
data_clean = pnc_data %>% drop_na(age)

# Assigning sample split number and fold number 
ss_num = 20
folds = 5

# Creating storage variables for hypertuning
lambda_struct = c()
lambda_func = c()

# Assigning storage matrices within data frame columns 
data_clean$struct_ss  <- matrix(nrow = nrow(data_clean), ncol = 0)
data_clean$func_ss  <- matrix(nrow = nrow(data_clean), ncol = 0)
data_clean$fold_ss  <- matrix(nrow = nrow(data_clean), ncol = 0)

# Running the sample splits
for(j in 1:ss_num){
  # Creating a temporary data frame
  data_temp <- data_clean
  
  # Creating storage columns
  data_temp$preds_struct = NA
  data_temp$preds_func = NA
  
  # Setting fold numbers for temporary data frame
  data_temp$fold <- sample(rep(1:folds, length.out = nrow(data_temp)))
  
  
  # Creating a for loop for 5-fold cross validation
  for(i in 1:folds){
    # Assigning data
    test = data_temp %>% filter(fold == i)
    train = data_temp %>% filter(fold != i)
    
    # Creating model data variables for training  
    x_var_struct = data.frame(train[, grepl('X17', colnames(train)) | colnames(train) %in% c(outcome, "sex_bin", "euler", "meanFD")])
    y_var = data.frame(train[, colnames(train) %in% c("age")])
    x_var_func = data.frame(train[, grepl('_to_', colnames(train)) | colnames(train) %in% c(outcome, "sex_bin", "euler", "meanFD")])
  
    # Creating model variables for prediction
    newx_struct = data.frame(test[, grepl('X17', colnames(test)) | colnames(test) %in% c("sex_bin", "euler", "meanFD")])
    newx_func = data.frame(test[, grepl('_to_', colnames(test)) | colnames(test) %in% c("sex_bin", "euler", "meanFD")])
    newy = data.frame(test[, colnames(test) %in% c("age")])
    
    # Creating random forest tasks
    task_struct = makeRegrTask(id = "rf_task", data = x_var_struct, target = outcome)
    task_func = makeRegrTask(id = "rf_task", data = x_var_func, target = outcome)
    
    # Tuning each model
    rfTune_struct = tuneRanger(task_struct, num.trees = 250, iters = 25, iters.warmup = 10, show.info = FALSE)
    rfTune_func = tuneRanger(task_func, num.trees = 250, iters = 25, iters.warmup = 10, show.info = FALSE)
    
    # Checking the tuning of each model
    rfTune_struct$model = helper_ranger(x_var_struct, outcome, newx_struct, rfTune_struct) # Calling for checking of tuning 
    rfTune_func$model = helper_ranger(x_var_func, outcome, newx_func, rfTune_func) # Calling for checking of tuning 
    
    # Getting predictions
    test$preds_struct = predict(rfTune_struct$model, newdata = newx_struct)$data$response
    test$preds_func = predict(rfTune_func$model, newdata = newx_func)$data$response
    
    # Storing prediction values
    ## Isolating to a singular data frame with only the prediction values and IDs
    test_collapsed <- test %>%
      group_by(participant_id) %>%
      summarise(
        preds_struct = first(na.omit(preds_struct)),
        preds_func = first(na.omit(preds_func)),
        .groups = "drop"
      )
    
    ## Matching the predictions by ID to the original data frame
    data_temp <- data_temp %>%
      left_join(test_collapsed, by = "participant_id", suffix = c("", "_new"))%>%
      mutate(
        preds_struct = ifelse(is.na(preds_struct_new), preds_struct, preds_struct_new),
        preds_func = ifelse(is.na(preds_func_new), preds_func, preds_func_new)
      ) %>%
      select(-ends_with("_new"))
  }
  
  # Storing results from each model 
  data_clean$struct_ss <- cbind(data_clean$struct_ss, as.matrix(data_temp$preds_struct))
  data_clean$func_ss <- cbind(data_clean$func_ss, as.matrix(data_temp$preds_func))
  data_clean$fold_ss <- cbind(data_clean$fold_ss, as.matrix(data_temp$fold))
}
```

### Applying mapa to Data Predictions
```{r}
# Gathering the estimates with four sample splits
results = mapa(data_clean$age, data_clean$struct_ss, data_clean$fold_ss, data_clean$func_ss)

# Here, yhat1 represents the prediction accuracy of the structural model and 
# yhat2 represents the prediction accuracy of the functional model 
results$est <- results$est %>% mutate(metric = recode(metric, yhat1 = "Structural Data", yhat2 = "Functional Data"))
results$est_summary <- results$est_summary %>% mutate(metric = recode(metric, yhat1 = "Structural Data", yhat2 = "Functional Data"))

# Previewing summarized results
results$est_summary
```

### Graphing Outputs
```{r}
# Filtering to only the estimates for the eTIV and nWBV estimates
toPlot <- results$est_summary %>% filter(metric == "Structural Data" | metric == "Functional Data")
toPlot_Diff <- results$est_summary %>% filter(metric == "Diff")

# Plotting one-step results 
ggplot(toPlot, aes(x = metric, y = est)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = LB, ymax = UB),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1)) + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")  + 
  labs(x = "Random Forest Model Data", y = "One-Step Estimator Value")



# Plotting one-step difference results 
ggplot(toPlot_Diff, aes(x = metric, y = est)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = LB, ymax = UB),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  theme_bw() + 
  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + 
  coord_flip() + 
  geom_hline(yintercept = 0, linetype = "dashed", color = "black")  + 
  ylim(-1,1) + 
  labs(x = "One-Step Difference", y = "Difference Magnitude")
```

### Analysis
Here, with the inclusion of sample splits to test other scenarios where fold assignment randomly differs, we see that the model trained on structural data does not perform significantly better than the model trained on functional data when predicting subject age. Further, the plausible range of difference in correlation is between 0.07 and 0.14 between the structural and functional MRI-trained models. 
























